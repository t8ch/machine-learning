{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "69bbf41d-1a30-4ea1-9bcc-41a8c52de069"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variational Autoencoders - an introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent variables and generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **idea**: data is generated by a two-step (stochastic process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- first, a low-dimensional and unobserved latent variable $z$ is determined (using $P(z)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- second, the observed data $x$ is generated via the decoder $P(x|z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- examples for latent variables: digit between 0 and 9; presented sensory stimulus\n",
    "- examples for observed data: image of handwritten digit; neural activity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$P(x) = \\int P(x|z)P(z) dz$$\n",
    "\n",
    "<center><img src=\"figs/gm1.jpg\" width=\"300\" align='middle'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**generative models**\n",
    "\n",
    "if an appropriate representation of $z$ and a decoder $P(x|z)$ can be found, imaginary data samples can be generated by sampling from them\n",
    "\n",
    "most popular class of such systems: generative adversarial networks (GAN; *Goodfellow, Ian, et al. \"Generative adversarial nets.\" NIPS, 2014.*)\n",
    "\n",
    "[example: artificially generated faces](http://thispersondoesnotexist.com)\n",
    "\n",
    "variational autoencoders (VAE) are another one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Autoencoders\n",
    "### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- general scheme of autoencoders:\n",
    "$$x\\longrightarrow z=f(x,\\theta) \\longrightarrow \\hat x = g(z,\\phi)$$\n",
    "- trying to optimize reconstruction $\\hat x$ (for example by minimizing mean squared error)\n",
    "- latent representation $z$ with lower dimensionality $\\Rightarrow$ compression/dimensionality reduction\n",
    "- for linear $f, g$: latent variables $z$ are projections on principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"figs/mnist-pca.png\" width=\"400\" align='middle'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Variational Autoencoders\n",
    "- VAE: $f$ and $g$ become probabilitic, non-deterministic functions that are implemented via neural networks \n",
    "<center><img src=\"figs/vae1.png\" width=\"400\" align='middle'></center>[picture source](https://blog.fastforwardlabs.com/2016/08/22/under-the-hood-of-the-variational-autoencoder-in.html)\n",
    "\n",
    "- the latent space (hopefully) provides a nice low-dimensional manifold of the data\n",
    "\n",
    "<center>But how are the network parameters and the latent space found?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The variational bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **problem**: the integral $p_\\theta(x)=\\int p_\\theta (z)p_\\theta(x|z)dz$ is intractable and cannot easily be maximized with respect to $\\theta$ <br>(same for posterior $p_\\theta(z|x)=p_\\theta(x|z)p_\\theta(z)/p_\\theta(x)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- one solution: Monte Carlo sampling $p_\\theta(x)\\approx \\frac{1}{N}\\sum_i p(x|z_i)$, **but** this requires many sampling points for each $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **variational inference**: approximate posterior $p_\\theta(z|x)$ by parametrized function $q_\\Phi(z|x)$, the (learnable) prior over $z$ by $p_\\theta(z)$, and obtain variational **evidence lower bound** (ELBO):\n",
    "<center><img src=\"figs/elbo.jpg\" width=\"630\" align='middle'></center> <small>[Kingma & Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).]</small>\n",
    "\n",
    "- **interpretation**: lower bound wants the best possible tradeoff of\n",
    "  - KL divergence of posterior and prior (minimize)\n",
    "  - log-likelihood of the observed data (maximize)\n",
    "- interesting note on Eq. (2): negative of lower bound is variational free enegery as known from statistical physics<br>\n",
    "$$-\\mathcal{L}=\\mathbb{E}_q\\left[E(x,z)\\right]-H(q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **variational inference**: approximate posterior $p_\\theta(z|x)$ by parametrized function $q_\\Phi(z|x)$, the (learnable) prior over $z$ by p_\\theta(z), and obtain variational **evidence lower bound** (ELBO):\n",
    "<center><img src=\"figs/elbo.jpg\" width=\"630\" align='middle'></center> <small>[Kingma & Welling. \"Auto-encoding variational bayes.\" arXiv preprint arXiv:1312.6114 (2013).]</small>\n",
    "\n",
    "- alternative interpretation 1: $\\mathcal{L}$= optimized reconstruction + $\\beta\\cdot$regularization.  ($\\beta$-VAE)\n",
    "- alternative interpretation 2: rate-distortion theory: $-\\mathcal{L}=$distortion + (information) rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (putative) advantages of random sampling from encoder\n",
    "- model is generative\n",
    "- latent space becomes smooth (data is denoised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **however**: \"VAEs\n",
    "tend to generate blurry samples, a condition which has been\n",
    "attributed to using overly simplistic distributions for the\n",
    "prior\" <small>[Gosh et al. From Variational to Deterministic Autoencoders.]</small>\n",
    "\n",
    "<center><figure>\n",
    "<img src=\"figs/kl-tradeoff.jpg\" width=\"400\" align='middle'>\n",
    "<figcaption>The tradeoff can be harmful for either reconstruction quality or sampling</figcaption>\n",
    "</figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- rate-distortion curve characterizes the tradeoff between compression and reconstruction accuracy\n",
    "<center><img src=\"figs/rd.jpg\" width=\"450\" align='middle'></center> <small>[Alemi et al. \"Fixing a broken ELBO.\" arXiv preprint arXiv:1711.00464 (2017).]</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VAE in practice\n",
    "- often isotropic Gaussians are assumed as priors: $p_\\theta(z)=\\mathcal{N}(z;0,\\mathbb{I})$\n",
    "- \" The key is to notice that any distribution in d dimensions can\n",
    "be generated by taking a set of d variables that are normally distributed and\n",
    "mapping them through a sufficiently complicated function.\" <br><small>Doersch, Carl. \"Tutorial on variational autoencoders.\" arXiv preprint arXiv:1606.05908 (2016).</small>\n",
    "- the KL divergence becomes analytic and does not have to be estimated\n",
    "\n",
    "<center><img src=\"figs/vae2.jpg\" width=\"600\" align='middle'></center><small>Doersch, Carl. \"Tutorial on variational autoencoders.\" arXiv preprint arXiv:1606.05908 (2016).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These principles are the same as used in the LFADS system discussed last time.\n",
    "\n",
    "let's see some code in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dde58264-c806-473a-890c-ed178f3d8287"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook vae-intro-slides.ipynb to slides\n",
      "[NbConvertApp] Writing 337135 bytes to vae-intro-slides.slides.html\n",
      "[NbConvertApp] Redirecting reveal.js requests to https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.5.0\n",
      "Serving your slides at http://127.0.0.1:8000/vae-intro-slides.slides.html\n",
      "Use Control-C to stop this server\n",
      "WARNING:tornado.access:404 GET /custom.css (127.0.0.1) 0.70ms\n",
      "WARNING:tornado.access:404 GET /custom.css (127.0.0.1) 0.97ms\n",
      "WARNING:tornado.access:404 GET /favicon.ico (127.0.0.1) 0.39ms\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert vae-intro-slides.ipynb --to slides --post serve --SlidesExporter.reveal_scroll=True "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
