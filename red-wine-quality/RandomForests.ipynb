{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "69bbf41d-1a30-4ea1-9bcc-41a8c52de069"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests and Ensembles\n",
    "\n",
    "<center>a brief introduction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2e1dd02f-f9b9-4cb8-a283-5efd45fcbb87"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## model averaging and stacking\n",
    "- \"Stacking [..] involves training a learning algorithm to combine the predictions of several other learning algorithms.\" (wiki)\n",
    "- Bayesian **averaging** of models $M_m$ (training data $Z$):\n",
    "$$E(\\text{pred}|Z) = \\sum_m E(\\text{pred}|Z,M_m)p(M_m|Z)$$\n",
    "- **stacking**: find best superposition of predictions from single learners (here regression)\n",
    "$$\\hat w^{st} = \\text{argmin}_{w} \\sum_i \\left[ y_i - \\sum_m w_m \\hat f_m^{-i}(x_i)\\right]^2 ,$$\n",
    "where $f_m^{-i}(x)$ is the prediction of $x$, using model $m$, applied to data set with $x_i$ removed. \n",
    "- related to leave-one-out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e39941e8-a962-45ec-b70b-876384093f44"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## bootstrapping\n",
    "-  generate distinct data sets by repeatedly sampling observations from the original data set with replacement\n",
    "- leverage learning for a given data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9f98f44c-dcf5-48dd-995d-ae683013ea80"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## bagging\n",
    "- bagging = \"bootstrap aggregating\"\n",
    "- averaging the predictions $\\hat f^{*b}$ of many learners (each with high variance) that were trained on bootstrap samples\n",
    "\n",
    "$$\\hat f_\\mathrm{bag} = \\frac{1}{B}\\sum_{b=1}^B \\hat f^{*b}(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6802077c-890a-47fa-8440-316eb1e4cd37"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### bagged trees\n",
    "- grow $B$ deep decision trees (on bootstrapped data) that have high variance but low bias\n",
    "- averaging reduces variance ($\\propto 1/n$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eec641bd-d7ac-40d8-896a-eb974e0db21d"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/bagging1.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6ee91354-0841-43e6-be52-e096a3383bc9"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/bagging2.jpg\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "73ff84a1-75e6-4536-8062-fca89223d4b8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random forests\n",
    "- similar to bagged trees but with less correlation among trees $\\Rightarrow$ less variance\n",
    "- at each split only $m$ out of $p$ features are chosen for selection (typically $\\sqrt{p}$)\n",
    "- trees become less similar as they cannot generally make the same split decisions\n",
    "<img src=\"figures/randomforest1.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "409709ff-6b38-48e4-85e4-8310553fd6e9"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### feature importance\n",
    "- **reminder**: trees are fitted by choosing splits that minimize\n",
    "    1. *Gini index* $G=\\sum_k p_{mk}(1-p_{mk})$ or,\n",
    "    2. *Entropy* $D=-\\sum_k p_{mk}\\log(p_{mk})$,\n",
    "  <br>where $p_{mk}$ is the fraction of labels $k$ in region $m$\n",
    "- feature importance of feature $x$ is measured as the \"summed reduction on the split criterion ($G$ or $E$) for all splits on $x$\" \n",
    "- feature randomization \"equalizes\" feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d6e8e507-2919-47ad-986f-b342c155e400"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting and boosted trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e8a87f44-aa91-4db6-8f67-ae830a5dab5b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting methods\n",
    "- another very powerful variant of *committee-based* methods (not only for trees)\n",
    "- **basic idea**: fit \"weak learners\" to the residuals of previous weak learners\n",
    "- AdaBoost algorithm: final prediction $G(x)$ is given by $G(x)=f\\left(\\sum_{m=1}^M \\alpha_mG_m(x)\\right)$, where $f()$ depends on the classification/regression task and $\\alpha_m$ is the weight of learner $G_m$.\n",
    "<img src=\"figures/AdaBoost-schematic.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bd01da00-f8e0-4017-bbf6-1de03d3f7aba"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "current classifier $G_m$ is weighted according to their error; weights of each data point for the next classifier are calculated in 2d.\n",
    "\n",
    "<img src=\"figures/AdaBoost.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5a588d68-8705-413e-9b55-92fb6c4e9eee"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosted trees\n",
    "tuning parameters:\n",
    "- number of trees $B$\n",
    "- shrinkage (learning rate) $\\lambda$\n",
    "- number of splits $d$ in each tree\n",
    "\n",
    "<img src=\"figures/BoostedTrees-algo.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "df9bc7aa-da8e-493b-9318-1c2c375aef86"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/randomforest-vs-boost.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5be4c4a-f233-40bb-8260-06803ead689f"
    }
   },
   "source": [
    "## Ensembles\n",
    "- \"The idea of ensemble learning is to build a prediction model by combining the strengths of a collection of simpler base models.\" $\\Rightarrow$ *all of the previous*\n",
    "- Ensemble learning can be broken down into two tasks: \n",
    "    - developing a population of base learners from the training data,\n",
    "    - and then combining them to form the composite predictor\n",
    "- ESL provides more technical, algoritmic information (chapter 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dde58264-c806-473a-890c-ed178f3d8287"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert RandomForests.ipynb --to slides --post serve --SlidesExporter.reveal_scroll=True "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
